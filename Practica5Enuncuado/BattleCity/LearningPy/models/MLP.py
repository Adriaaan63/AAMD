import numpy as np

class MLP:

    """
    Constructor: Computes MLP.

    Args:
        inputLayer (int): size of input
        hiddenLayer (int): size of hidden layer.
        outputLayer (int): size of output layer
        seed (scalar): seed of the random numeric.
        epislom (scalar) : random initialization range. e.j: 1 = [-1..1], 2 = [-2,2]...
    """

    def __init__(self,input_layer_size,hidden_layer_size, output_layer_size, seed=0, epsilom = 0.12):
        np.random.seed(seed)
        self.seed = seed
        self.epsilom = epsilom

        ## TO-DO
        self.layer_sizes = [input_layer_size] + hidden_layer_size + [output_layer_size]

        # Inicializar pesos para cada capa. Matrices de pesos(thetas) 
        self.thetas = []

        for i in range(len(self.layer_sizes) - 1):
            ori = self.layer_sizes[i]
            dest = self.layer_sizes[i + 1]

            theta = np.random.uniform(-epsilom, epsilom, (dest, ori + 1))
            self.thetas.append(theta)
        """
    Reset the theta matrix created in the constructor by both theta matrix manualy loaded.

    Args:
        theta1 (array_like): Weights for the first layer in the neural network.
        theta2 (array_like): Weights for the second layer in the neural network.
    """
    def new_trained(self,theta1,theta2):
        self.theta1 = theta1
        self.theta2 = theta2
        
    """
    Num elements in the training data. (private)

    Args:
        x (array_like): input data. 
    """
    def _size(self,x):
        return x.shape[0]
    
    """
    Computes de sigmoid function of z (private)

    Args:
        z (array_like): activation signal received by the layer.
    """
    def _sigmoid(self,z):
        return 1.0 / (1.0 + np.exp(-z))

    """
    Computes de sigmoid derivation of de activation (private)

    Args:
        a (array_like): activation received by the layer.
    """   
    def _sigmoidPrime(self,a):
        return a * (1 - a)


    def _sigmoidGradient(self,z):
        g = self._sigmoid(z)
        return g * (1 - g)
    """
    Run the feedwordwar neural network step

    Args:
        x (array_like): input of the neural network.

	Return 
	------
	As (array_like): activation functions of each layer
    Zs (array_like): signal fuction of each layer
    """

    #n cada iteración del entrenamiento se repite este ciclo: Feedforward -> Error -> Backpropagation -> Actualización de pesos
    def feedforward(self,x):
         m = self._size(x)

         a_curr = x
         As = [a_curr]  # Lista para almacenar las activaciones de cada capa
         Zs = []        # Lista para almacenar los valores z de cada capa
         for i in range(len(self.thetas)):
             a_curr = np.hstack((np.ones((m, 1)), a_curr))  # Añadir el término de sesgo
             z_curr = a_curr @ self.thetas[i].T
             Zs.append(z_curr)
             a_curr = self._sigmoid(z_curr)
             As.append(a_curr)
         
         return As, Zs


    """
    Computes only the cost of a previously generated output (private)

    Args:
        yPrime (array_like): output generated by neural network.
        y (array_like): output from the dataset
        lambda_ (scalar): regularization parameter

	Return 
	------
	J (scalar): the cost.
    """
    def compute_cost(self, yPrime,y, lambda_): 
        m = y.shape[0]                                                  #número de ejemplos de entrenamiento
        eps = 1e-12                                                     # Define un valor mínimo para evitar que np.log(0) dé error o infinito.
        yhat = np.clip(yPrime, eps, 1-eps)                              # Asegura que todas las predicciones estén entre [1e-12, 1 - 1e-12]. Así, nunca calculas log(0) o log(1) exacto
        term = - ( y * np.log(yhat) + (1 - y) * np.log(1 - yhat) )      #Calcula el error de cada ejemplo y cada salida según la fórmula de entropía cruzada
        J = (1.0 / m) * np.sum(term)
        J += self._regularizationL2Cost(m, lambda_)                     #Hace el promedio, añadiendole la regularizacion L2 (penalización por pesos grandes)
        return J
    

    """
    Get the class with highest activation value

    Args:
        a3 (array_like): output generated by neural network.

	Return 
	------
	p (scalar): the class index with the highest activation value.
    """
    def predict(self, output_activations):
        p = np.argmax(output_activations, axis=1)
        return p
    

    """
    Compute the gradients of both theta matrix parámeters and cost J

    Args:
        x (array_like): input of the neural network.
        y (array_like): output of the neural network.
        lambda_ (scalar): regularization.

	Return 
	------
	J: cost
    grad1, grad2: the gradient matrix (same shape than theta1 and theta2)
    """
    def compute_gradients(self, x, y, lambda_):
        m = x.shape[0]
        # forward
        As, Zs = self.feedforward(x)
        a_last = As[-1]  # Activaciones de la capa de salida

        # Calculo del coste(error total) (incluye regularización) 
        J = self.compute_cost(a_last, y, lambda_)

        # backpropagation
        grads = [np.zeros_like(theta) for theta in self.thetas]
        # Error en la capa de salida
        delta_last = a_last - y

        # Lista de deltas
        deltas = [delta_last]

        # Bucle hacia atrás para calcular los deltas de las capas ocultas
        for i in range(len(self.thetas) - 1, 0, -1):
            theta_curr = self.thetas[i]
            delta_next = deltas[0]
            a_prev = As[i]

            delta_curr = delta_next.dot(theta_curr[:, 1:]) * self._sigmoidPrime(a_prev)
            deltas.insert(0, delta_curr)

        for i in range(len(self.thetas)):
            delta = deltas[i]
            a_prev = As[i]

            #Se añade el bias a la activación de la capa anterior
            a_prev_bias = np.hstack((np.ones((m, 1)), a_prev))
            #Gradiente sin regularizar para la capa i
            grads[i] = (1.0 / m) * (delta.T @ a_prev_bias)
            # se añade la regularización L2
            grads[i] += self._regularizationL2Gradient(self.thetas[i], lambda_, m)

        return J, grads
    
    """
    Compute L2 regularization gradient

    Args:
        theta (array_like): a theta matrix to calculate the regularization.
        lambda_ (scalar): regularization.
        m (scalar): the size of the X examples.

	Return 
	------
	L2 Gradient value
    """
    def _regularizationL2Gradient(self, theta, lambda_, m):
        reg = np.zeros_like(theta)
        reg[:, 1:] = (lambda_ / m) * theta[:, 1:]   # no regulariza los bias
        return reg
    
    
    """
    Compute L2 regularization cost

    Args:
        lambda_ (scalar): regularization.
        m (scalar): the size of the X examples.

	Return 
	------
	L2 cost value
    """

    def _regularizationL2Cost(self, m, lambda_):
        if lambda_ == 0:
            return 0.0
        total_sum = 0
        for theta in self.thetas:
            total_sum += np.sum(theta[:, 1:] ** 2) 
        return (lambda_ / (2.0 * m)) * total_sum
    
    
    def backpropagation(self, x, y, alpha, lambda_, numIte, verbose=0):
        Jhistory = []
        for i in range(numIte):
            ##calculate gradients and update both theta matrix
            J, grad = self.compute_gradients(x, y, lambda_)

            for j in range(len(self.thetas)):
                self.thetas[j] = self.thetas[j] - alpha * grad[j]

            Jhistory.append(J)
            if verbose > 0 :
                if i % verbose == 0 or i == (numIte-1):
                    print(f"Iteration {(i+1):6}: Cost {float(J):8.4f}   ")
        return Jhistory
    


"""
target_gradient funcitón of gradient test 1
"""
def target_gradient(input_layer_size,hidden_layer_size,num_labels,x,y,reg_param):
    mlp = MLP(input_layer_size,hidden_layer_size,num_labels)
    J, grad1, grad2 = mlp.compute_gradients(x,y,reg_param)
    return J, grad1, grad2, mlp.theta1, mlp.theta2


"""
costNN funcitón of gradient test 1
"""
def costNN(Theta1, Theta2,x, ys, reg_param):
    mlp = MLP(x.shape[1],1, ys.shape[1])
    mlp.new_trained(Theta1,Theta2)
    J, grad1, grad2 = mlp.compute_gradients(x,ys,reg_param)
    return J, grad1, grad2


"""
mlp_backprop_predict 2 to be execute test 2
"""
def MLP_backprop_predict(X_train,y_train, X_test, alpha, lambda_, num_ite, verbose):
    mlp = MLP(X_train.shape[1],25,y_train.shape[1])
    Jhistory = mlp.backpropagation(X_train,y_train,alpha,lambda_,num_ite,verbose)
    a3= mlp.feedforward(X_test)[2]
    y_pred=mlp.predict(a3)
    return y_pred